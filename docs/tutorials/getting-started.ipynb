{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67596aca",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "## Installation\n",
    "\n",
    "```\n",
    "pip install pypekit\n",
    "```\n",
    "\n",
    "## Define Tasks\n",
    "\n",
    "To define a task, create a subclass of `Task`, override the `input_types` and `output_types` properties, and implement the `run` method. \n",
    "\n",
    "`input_types` and `output_types` are lists of strings that represent the types of inputs and outputs for the task \n",
    "and the `run` method contains the logic for processing the inputs and producing the outputs.\n",
    "\n",
    "When we later use these tasks to create pipelines, they will run from tasks with the input type `\"source\"` to tasks with the output type `\"sink\"`.\n",
    "All other input and output types are intermediate types, that you can use to define how tasks are connected together.\n",
    "\n",
    "For two tasks to be connected, at least one of the output types of the first task must match one of the input types of the second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a0f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypekit import Task\n",
    "\n",
    "class Source(Task):\n",
    "    input_types = [\"source\"]\n",
    "    output_types = [\"a\"]\n",
    "\n",
    "    def run(self, _):\n",
    "        print(\"Running Source\")\n",
    "        return \"source\"\n",
    "\n",
    "class Transform1(Task):\n",
    "    input_types = [\"a\"]\n",
    "    output_types = [\"b\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Transform1\")\n",
    "        return x + \"_transformed-1\"\n",
    "    \n",
    "class Transform2(Task):\n",
    "    input_types = [\"a\", \"b\"]\n",
    "    output_types = [\"b\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Transform2\")\n",
    "        return x + \"_transformed-2\"\n",
    "    \n",
    "class Sink1(Task):\n",
    "    input_types = [\"b\"]\n",
    "    output_types = [\"sink\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Sink1\")\n",
    "        return x + \"_sink-1\"\n",
    "    \n",
    "class Sink2(Task):\n",
    "    input_types = [\"b\"]\n",
    "    output_types = [\"sink\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Sink2\")\n",
    "        return x + \"_sink-2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67b003",
   "metadata": {},
   "source": [
    "## Build Repository\n",
    "\n",
    "To create all possible pipelines from a list of tasks, we now create a `Repository`.\n",
    "We can then use the `build_tree()` method to build a tree of all possible pipelines from the tasks defined in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f54657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypekit import Repository\n",
    "\n",
    "repository = Repository([\n",
    "    Source,\n",
    "    Transform1,\n",
    "    Transform2,\n",
    "    Sink1,\n",
    "    Sink2\n",
    "])\n",
    "\n",
    "root = repository.build_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8f2f9",
   "metadata": {},
   "source": [
    "## Inspect Tree\n",
    "\n",
    "To inspect the tree structure, we can use the `build_tree_string()` method, which will return a string representation of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bade5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── Root()\n",
      "    └── Source()\n",
      "        ├── Transform1()\n",
      "        │   ├── Transform2()\n",
      "        │   │   ├── Sink1()\n",
      "        │   │   └── Sink2()\n",
      "        │   ├── Sink1()\n",
      "        │   └── Sink2()\n",
      "        └── Transform2()\n",
      "            ├── Sink1()\n",
      "            └── Sink2()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_representation = repository.build_tree_string()\n",
    "print(tree_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194265d",
   "metadata": {},
   "source": [
    "## Build Pipelines\n",
    "\n",
    "With the `build_pipelines()` method, we can build all possible pipelines from the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122b038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(tasks=[Source(), Transform1(), Transform2(), Sink1()])\n",
      "Pipeline(tasks=[Source(), Transform1(), Transform2(), Sink2()])\n",
      "Pipeline(tasks=[Source(), Transform1(), Sink1()])\n",
      "Pipeline(tasks=[Source(), Transform1(), Sink2()])\n",
      "Pipeline(tasks=[Source(), Transform2(), Sink1()])\n",
      "Pipeline(tasks=[Source(), Transform2(), Sink2()])\n"
     ]
    }
   ],
   "source": [
    "pipelines = repository.build_pipelines()\n",
    "for p in pipelines:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b058404",
   "metadata": {},
   "source": [
    "## Execute Pipelines with Caching\n",
    "\n",
    "Running many similar pipelines can be wasteful if they share sub-chains. \n",
    "To cache intermediate results, we can use the `CachedExecutor`. \n",
    "Simply pass a list of pipelines to the `CachedExecutor` and call the `run()` method.\n",
    "As you can see in the output, the executor only runs the tasks that are not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860359c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Source\n",
      "Running Transform1\n",
      "Running Transform2\n",
      "Running Sink1\n",
      "Pipeline 1/6 completed. Runtime: 0.00s.\n",
      "Running Sink2\n",
      "Pipeline 2/6 completed. Runtime: 0.00s.\n",
      "Running Sink1\n",
      "Pipeline 3/6 completed. Runtime: 0.00s.\n",
      "Running Sink2\n",
      "Pipeline 4/6 completed. Runtime: 0.00s.\n",
      "Running Transform2\n",
      "Running Sink1\n",
      "Pipeline 5/6 completed. Runtime: 0.00s.\n",
      "Running Sink2\n",
      "Pipeline 6/6 completed. Runtime: 0.00s.\n"
     ]
    }
   ],
   "source": [
    "from pypekit import CachedExecutor\n",
    "\n",
    "executor = CachedExecutor(pipelines, verbose=True)\n",
    "results = executor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617012a7",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "\n",
    "After `run()` finishes, `executor.results` is a nested dict whose keys are pipeline IDs.\n",
    "Each entry records:\n",
    "\n",
    "* **`output`** – the final value produced,\n",
    "* **`runtime`** – cumulative seconds spent (for cached tasks, the runtime is also taken from the cache),\n",
    "* **`tasks`** – the human-readable task list that formed the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb407c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"output\": \"source_transformed-1_transformed-2_sink-1\",\n",
      "  \"runtime\": 7.836699933250202e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink1()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-1_transformed-2_sink-2\",\n",
      "  \"runtime\": 8.094199893093901e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink2()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-1_sink-1\",\n",
      "  \"runtime\": 7.130399899324402e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Sink1()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-1_sink-2\",\n",
      "  \"runtime\": 7.185499907791382e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Sink2()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-2_sink-1\",\n",
      "  \"runtime\": 6.648499856964918e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink1()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-2_sink-2\",\n",
      "  \"runtime\": 6.663499880232848e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink2()\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for r in results.values():\n",
    "    print(json.dumps(r, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8b226",
   "metadata": {},
   "source": [
    "# Reusing Cache\n",
    "\n",
    "If you already have a cache from a previous run, you can reuse it by passing the `cache` argument to the `CachedExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4ced3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1/6 completed. Runtime: 0.00s.\n",
      "Pipeline 2/6 completed. Runtime: 0.00s.\n",
      "Pipeline 3/6 completed. Runtime: 0.00s.\n",
      "Pipeline 4/6 completed. Runtime: 0.00s.\n",
      "Pipeline 5/6 completed. Runtime: 0.00s.\n",
      "Pipeline 6/6 completed. Runtime: 0.00s.\n"
     ]
    }
   ],
   "source": [
    "new_executor = CachedExecutor(pipelines, cache=executor.cache, verbose=True)\n",
    "new_executor.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276873a",
   "metadata": {},
   "source": [
    "# Instances, Parameters and Pipelines\n",
    "\n",
    "A repository can mix and match several flavours of “tasks”:\n",
    "\n",
    "| What you pass                                | How the repository treats it                          |\n",
    "| -------------------------------------------- | ----------------------------------------------------- |\n",
    "| A **class**                                  | Instantiated at every node of the tree.               |\n",
    "| An **instance** (`Task()`)                   | Re-use the same instance everywhere it’s needed.      |\n",
    "| A **tuple** (`Task`, `kwargs`)               | Task class with kwargs to use on instantiation.       |\n",
    "| An existing **Pipeline**                     | Used as an instance of a task.                        |\n",
    "\n",
    "This flexibility lets you\n",
    "\n",
    "* reuse heavyweight objects (e.g. a loaded ML model),\n",
    "* scan hyper-parameters by specifying multiple (class, kwargs) tuples,\n",
    "* embed pre-fabricated sub-pipelines inside larger graphs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a32e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── Root()\n",
      "    └── Source()\n",
      "        ├── Transform1()\n",
      "        │   └── Sink1()\n",
      "        ├── Transform3(test=True)\n",
      "        │   └── Sink1()\n",
      "        └── Pipeline(tasks=[Transform1(), Sink2()])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypekit import Pipeline\n",
    "\n",
    "# New task, that takes arguments\n",
    "class Transform3(Task):\n",
    "    input_types = [\"a\"]\n",
    "    output_types = [\"b\"]\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.test = kwargs.get(\"test\", False)\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Transform3 with test =\", self.test)\n",
    "        return x + \"_transformed-3\" + (\"-test\" if self.test else \"\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    Transform1(),\n",
    "    Sink2()\n",
    "])\n",
    "\n",
    "repository = Repository([\n",
    "    Source,\n",
    "    Transform1,\n",
    "    (Transform3, {\"test\": True}),   # Transform3 will be instantiated with the argument test=True every time it is used\n",
    "    Sink1(),                        # Every node with the task Sink1 will have the same instance\n",
    "    pipeline                        # Pipeline instance as task\n",
    "])\n",
    "\n",
    "repository.build_tree()\n",
    "print(repository.build_tree_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypekit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
